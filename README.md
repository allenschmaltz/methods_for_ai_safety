# Introspection, Updatability, and Uncertainty Quantification with Transformers: Concrete Methods for AI Safety

Allen Schmaltz and Danielle Rasooly. "Introspection, Updatability, and Uncertainty Quantification with Transformers: Concrete Methods for AI Safety". *The ML Safety Workshop at the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)*, December 2022.

## Abstract

When deploying Transformer networks, we seek the ability to *introspect* the predictions against instances with known labels; *update* the model without a full re-training; and provide reliable *uncertainty* quantification over the predictions. We demonstrate that these properties are achievable via recently proposed approaches for approximating deep neural networks with instance-based metric learners, at varying resolutions of the input, and the associated **Venn-ADMIT** Predictor for constructing prediction sets. We consider a challenging (but non-adversarial) task: Zero-shot sequence labeling (i.e., feature detection) in a low-accuracy, class-imbalanced, covariate-shifted setting while requiring a high confidence level.

## Paper

[https://openreview.net/pdf?id=w9U_7Ay7f86](https://openreview.net/pdf?id=w9U_7Ay7f86)

## Presentations

*The ML Safety Workshop at the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)*. Virtual. December 9, 2022.
- [poster](presentations/neurips22_ml_safety_workshop/final_neurips_workshop_2022_poster.pdf)
